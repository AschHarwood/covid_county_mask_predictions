{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.2.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: tensorflow>=2.2.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from scikeras) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from scikeras) (0.23.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.12)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (0.35.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (2.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (0.3.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.12.1)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (0.11.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.32.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.15.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (3.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorflow>=2.2.0->scikeras) (3.7.4.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from scikit-learn>=0.22.0->scikeras) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from scikit-learn>=0.22.0->scikeras) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from scikit-learn>=0.22.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (50.3.0.post20201103)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (1.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (1.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (3.3.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /anaconda/envs/covid_env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (4.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/covid_env/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/covid_env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->scikeras) (3.4.0)\n",
      "Installing collected packages: scikeras, numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "fbprophet 0.5 requires setuptools-git>=1.2, which is not installed.\n",
      "tensorflow-gpu 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\n",
      "tensorflow-gpu 2.3.1 requires tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.19.5 scikeras-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-hub\n",
    "import tensorflow_hub as hub\n",
    "import datetime, os\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from keras.losses import BinaryCrossentropy\n",
    "import talos\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "pd.set_option('max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in and process data\n",
    "df = pd.read_csv('/home/aschharwood/notebooks/covid/feature_target_v1_county_nyt_cdc_moa_tweets_gdelt_apple.csv')\n",
    "\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "#df.info(max_cols=500)\n",
    "\n",
    "mean = df._get_numeric_data().mean()\n",
    "\n",
    "#fill empty mobility data with column mean\n",
    "df.fillna(mean, inplace=True)\n",
    "\n",
    "#df.head()\n",
    "\n",
    "df['COUNTYFP'] = df['COUNTYFP'].astype('string')\n",
    "\n",
    "#df.describe()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#create binary target\n",
    "df['always_binary'] = np.where(df['ALWAYS']>.50, 1, 0)\n",
    "\n",
    "df['always_binary'].value_counts()\n",
    "df.fillna(df._get_numeric_data().mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up text data\n",
    "df['text_tokens_str'] = df['text_tokens_str'].replace(r'\\n',' ', regex=True)\n",
    "df['High'] = df['High'].replace(r'\\n',' ', regex=True)\n",
    "df['Low'] = df['Low'].replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discard unneeded mask wearing columns\n",
    "df.drop(['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS', 'FIPS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['always_binary']\n",
    "X = df.drop('always_binary', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab spacy's language model\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and load spacy language model\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load(disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help text tokenization function\n",
    "def tokenizer(text, nlp):\n",
    "\n",
    "    token_list = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_punct==False and token.like_url==False:\n",
    "            if token.text != ' ':\n",
    "                token_list.append((token.lemma_).lower())\n",
    "    str_tokens = ' '.join(token_list)\n",
    "    return str_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googles News Stacked\n",
    "\n",
    "- This model combines numeric inputs from CDC Social Vulnerability Index, Measure of America Youth Disconnection Index, and Apple mobility data at the county-level, with county-level geolocated tweets and state geotagged Covid data. It doesn't work very at the moment, but when I run each part of the model independently, I'm starting to see results. So I have still have some debugging to do. Nevertheless, this represents an initial attempt at building a stacked neural network architecutre that combines text data with word embeddings and numeric metadata.\n",
    "\n",
    "- The target for this model is binary classification about whether more or less than 50 percent of the population for each county wear's a mask. It's derived from the New York Times July 2020 survey into mask wearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['always_binary']\n",
    "X = df.drop('always_binary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = pd.get_dummies(X[['COUNTYFP', 'ST_ABBR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, cats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['High'] = X['High'].apply(lambda x: tokenizer(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Low'] = X['Low'].apply(lambda x: tokenizer(x, nlp))\n",
    "X['text_tokens_str'] = X['text_tokens_str'].apply(lambda x: tokenizer(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "#X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "X2_test = scaler.transform(X2_test)\n",
    "\n",
    "#download and set word vector pretrained model\n",
    "#embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "#hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "print('Google news model downlownded')\n",
    "\n",
    "X_News_High_train = X_train['High']\n",
    "X_News_High_test = X_test['High']\n",
    "\n",
    "X_News_Low_train = X_train['Low']\n",
    "X_News_Low_test = X_test['Low']\n",
    "\n",
    "X_tweet_train = X_train['text_tokens_str']\n",
    "X_tweet_test = X_test['text_tokens_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#numeric input\n",
    "num_input = Input(shape=(214,))\n",
    "dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "batch_out_num = tf.keras.layers.BatchNormalization()(dense_layer_1_num) \n",
    "num_output = Dense(10, activation='relu')(batch_out_num)\n",
    "\n",
    "#tweets low\n",
    "tweets = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "dense_1_tw = Dense(16, activation='relu')(hub_layer_tw)\n",
    "dense_2_tw = Dense(8, activation='relu')(dense_1_tw)\n",
    "batch_out_tweets = tf.keras.layers.BatchNormalization()(dense_2_tw) \n",
    "tw_output = Dense(4, activation='relu')(batch_out_tweets)\n",
    "#tw_output = Dropout(0.5)(dense_3_tw)\n",
    "\n",
    "#news low\n",
    "news_low = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_nl = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_low)\n",
    "dense_1_nl = Dense(16, activation='relu')(hub_layer_nl)\n",
    "dense_2_nl = Dense(8, activation='relu')(dense_1_nl)\n",
    "batch_out_nl = tf.keras.layers.BatchNormalization()(dense_2_nl) \n",
    "nl_output = Dense(4, activation='relu')(batch_out_nl)\n",
    "#nl_output = Dropout(0.5)(dense_3_nl)\n",
    "\n",
    "#news high\n",
    "news_high = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_news_high = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_high)\n",
    "dense_1_news_high = Dense(16, activation='relu')(hub_layer_news_high)\n",
    "dense_2_nh = Dense(8, activation='relu')(dense_1_news_high)\n",
    "batch_out_nh = tf.keras.layers.BatchNormalization()(dense_2_nh) \n",
    "nh_output = Dense(4, activation='relu')(batch_out_nh)\n",
    "#nh_output = Dropout(0.5)(dense_3_nh)\n",
    "\n",
    "\n",
    "#concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "concat_layer = Concatenate()([num_output, tw_output, nl_output, nh_output])\n",
    "# dense_1_cl = Dense(100, activation='relu')(concat_layer)\n",
    "# dense_2_cl = Dense(80, activation='relu')(dense_1_cl)\n",
    "# dense_3_cl = Dense(40, activation='relu')(dense_2_cl)\n",
    "# dense_4_cl = Dense(40, activation='relu')(dense_3_cl)\n",
    "output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "model = Model(inputs=[num_input, tweets, news_low, news_high], outputs=output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "\n",
    "validation_data=([X2_test, X_tweet_test, X_News_Low_test, X_News_High_test], y_test)\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=[X2_train, X_tweet_train, X_News_Low_train, X_News_High_train], y=y_train, epochs=5, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google news model downlownded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "#X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "X2_test = scaler.transform(X2_test)\n",
    "\n",
    "#download and set word vector pretrained model\n",
    "#embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "#hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "print('Google news model downlownded')\n",
    "\n",
    "X_News_High_train = X_train['High']\n",
    "X_News_High_test = X_test['High']\n",
    "\n",
    "X_News_Low_train = X_train['Low']\n",
    "X_News_Low_test = X_test['Low']\n",
    "\n",
    "X_tweet_train = X_train['text_tokens_str']\n",
    "X_tweet_test = X_test['text_tokens_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google news model downlownded\n",
      "Epoch 1/3\n",
      "79/79 [==============================] - 33s 419ms/step - loss: 0.6218 - accuracy: 0.7409\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 33s 420ms/step - loss: 0.5926 - accuracy: 0.8030\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 33s 423ms/step - loss: 0.5829 - accuracy: 0.8301\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.5922 - accuracy: 0.7933\n",
      "accuracy: 79.33%\n",
      "Google news model downlownded\n",
      "Epoch 1/3\n",
      "79/79 [==============================] - 35s 449ms/step - loss: 0.6193 - accuracy: 0.7501\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 36s 454ms/step - loss: 0.5893 - accuracy: 0.8078\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 36s 456ms/step - loss: 0.5907 - accuracy: 0.8102\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 0.6002 - accuracy: 0.7568\n",
      "accuracy: 75.68%\n",
      "Google news model downlownded\n",
      "Epoch 1/3\n",
      "79/79 [==============================] - 34s 435ms/step - loss: 0.6228 - accuracy: 0.7362\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 34s 435ms/step - loss: 0.5906 - accuracy: 0.8050\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 34s 428ms/step - loss: 0.5864 - accuracy: 0.8162\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.6072 - accuracy: 0.8013\n",
      "accuracy: 80.13%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-6bf1a02fc97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mcvscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f%% (+/- %.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "    X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "    #X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "    X2_test = scaler.transform(X2_test)\n",
    "\n",
    "    #download and set word vector pretrained model\n",
    "    #embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "    #hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "    embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "    print('Google news model downlownded')\n",
    "\n",
    "    X_News_High_train = X_train['High']\n",
    "    X_News_High_test = X_test['High']\n",
    "\n",
    "    X_News_Low_train = X_train['Low']\n",
    "    X_News_Low_test = X_test['Low']\n",
    "\n",
    "    X_tweet_train = X_train['text_tokens_str']\n",
    "    X_tweet_test = X_test['text_tokens_str']\n",
    "    \n",
    "    #numeric input\n",
    "    num_input = Input(shape=(214,))\n",
    "    dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "    batch_out_num = tf.keras.layers.BatchNormalization()(dense_layer_1_num) \n",
    "    num_output = Dense(10, activation='relu')(batch_out_num)\n",
    "\n",
    "    #tweets low\n",
    "    tweets = Input(shape=[], dtype=tf.string)\n",
    "    hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "    dense_1_tw = Dense(16, activation='relu')(hub_layer_tw)\n",
    "    dense_2_tw = Dense(8, activation='relu')(dense_1_tw)\n",
    "    batch_out_tweets = tf.keras.layers.BatchNormalization()(dense_2_tw) \n",
    "    tw_output = Dense(4, activation='relu')(batch_out_tweets)\n",
    "    #tw_output = Dropout(0.5)(dense_3_tw)\n",
    "\n",
    "    #news low\n",
    "    news_low = Input(shape=[], dtype=tf.string)\n",
    "    hub_layer_nl = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_low)\n",
    "    dense_1_nl = Dense(16, activation='relu')(hub_layer_nl)\n",
    "    dense_2_nl = Dense(8, activation='relu')(dense_1_nl)\n",
    "    batch_out_nl = tf.keras.layers.BatchNormalization()(dense_2_nl) \n",
    "    nl_output = Dense(4, activation='relu')(batch_out_nl)\n",
    "    #nl_output = Dropout(0.5)(dense_3_nl)\n",
    "\n",
    "    #news high\n",
    "    news_high = Input(shape=[], dtype=tf.string)\n",
    "    hub_layer_news_high = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_high)\n",
    "    dense_1_news_high = Dense(16, activation='relu')(hub_layer_news_high)\n",
    "    dense_2_nh = Dense(8, activation='relu')(dense_1_news_high)\n",
    "    batch_out_nh = tf.keras.layers.BatchNormalization()(dense_2_nh) \n",
    "    nh_output = Dense(4, activation='relu')(batch_out_nh)\n",
    "    #nh_output = Dropout(0.5)(dense_3_nh)\n",
    "\n",
    "\n",
    "    #concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "    concat_layer = Concatenate()([num_output, tw_output, nl_output, nh_output])\n",
    "    # dense_1_cl = Dense(100, activation='relu')(concat_layer)\n",
    "    # dense_2_cl = Dense(80, activation='relu')(dense_1_cl)\n",
    "    # dense_3_cl = Dense(40, activation='relu')(dense_2_cl)\n",
    "    # dense_4_cl = Dense(40, activation='relu')(dense_3_cl)\n",
    "    output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "    model = Model(inputs=[num_input, tweets, news_low, news_high], outputs=output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    model.fit(x=[X2_train, X_tweet_train, X_News_Low_train, X_News_High_train], y=y_train, epochs=3, callbacks=callbacks, verbose=1)\n",
    "    validation_data=([X2_test, X_tweet_test, X_News_Low_test, X_News_High_test], y_test)\n",
    "    scores = model.evaluate([X2_test, X_tweet_test, X_News_Low_test, X_News_High_test], y_test)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1]*100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.38% (+/- 1.94%)\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter Tuning with Keras Tuner and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Num Only\n",
    "\n",
    "- this model performs reasonably well with just the static socioeconomic and demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n",
      "training model\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.6709 - val_loss: 0.6247 - val_accuracy: 0.7154\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6019 - accuracy: 0.7887 - val_loss: 0.6036 - val_accuracy: 0.7901\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.8166 - val_loss: 0.5982 - val_accuracy: 0.7997\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5845 - accuracy: 0.8277 - val_loss: 0.5951 - val_accuracy: 0.8092\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5844 - accuracy: 0.8241 - val_loss: 0.5936 - val_accuracy: 0.8172\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.8245 - val_loss: 0.5952 - val_accuracy: 0.8029\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5837 - accuracy: 0.8237 - val_loss: 0.5942 - val_accuracy: 0.8076\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.8325 - val_loss: 0.5938 - val_accuracy: 0.8156\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5807 - accuracy: 0.8281 - val_loss: 0.6049 - val_accuracy: 0.8124\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5801 - accuracy: 0.8321 - val_loss: 0.6113 - val_accuracy: 0.8013\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "num_input = Input(shape=(214,))\n",
    "dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "num_output = Dense(1, activation='sigmoid')(dense_layer_1_num)\n",
    "model = Model(inputs=[num_input], outputs=num_output)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "validation_data=([X2_test], y_test)\n",
    "x=[X2_train]\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=x, y=y_train, epochs=10, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 214)]             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2150      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,161\n",
      "Trainable params: 2,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Debug - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254682d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254682d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254687440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254687440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n",
      "training model\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6692 - accuracy: 0.6403 - val_loss: 0.6575 - val_accuracy: 0.6455\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6380 - accuracy: 0.6836 - val_loss: 0.6532 - val_accuracy: 0.6455\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6268 - accuracy: 0.6976 - val_loss: 0.6560 - val_accuracy: 0.6216\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6225 - accuracy: 0.7027 - val_loss: 0.6559 - val_accuracy: 0.6375\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6201 - accuracy: 0.7083 - val_loss: 0.6590 - val_accuracy: 0.6264\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6178 - accuracy: 0.7131 - val_loss: 0.6575 - val_accuracy: 0.6312\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6171 - accuracy: 0.7139 - val_loss: 0.6578 - val_accuracy: 0.6328\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6165 - accuracy: 0.7155 - val_loss: 0.6589 - val_accuracy: 0.6264\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6160 - accuracy: 0.7163 - val_loss: 0.6581 - val_accuracy: 0.6296\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6160 - accuracy: 0.7163 - val_loss: 0.6580 - val_accuracy: 0.6296\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "tweets = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "dense_1_tw = Dense(16, activation='relu')(hub_layer_tw)\n",
    "dense_2_tw = Dense(8, activation='relu')(dense_1_tw)\n",
    "tw_output = Dense(1, activation='sigmoid')(dense_2_tw)\n",
    "\n",
    "model = Model(inputs=[tweets], outputs=tw_output)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "validation_data=([X_tweet_test], y_test)\n",
    "x=[X_tweet_train]\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=x, y=y_train, epochs=10, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model debug - stacke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550ca680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550ca680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550cad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550cad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n",
      "training model\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 16s 202ms/step - loss: 0.6497 - accuracy: 0.6486 - val_loss: 0.6471 - val_accuracy: 0.6455\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.6079 - accuracy: 0.7632 - val_loss: 0.6076 - val_accuracy: 0.7568\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 16s 200ms/step - loss: 0.5830 - accuracy: 0.8253 - val_loss: 0.6543 - val_accuracy: 0.6057\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.5771 - accuracy: 0.8333 - val_loss: 0.6339 - val_accuracy: 0.6725\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.5742 - accuracy: 0.8472 - val_loss: 0.6401 - val_accuracy: 0.6614\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "\n",
    "#numeric input\n",
    "num_input = Input(shape=(214,))\n",
    "dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "batch_out_num = tf.keras.layers.BatchNormalization()(dense_layer_1_num) \n",
    "num_output = Dense(10, activation='relu')(batch_out_num)\n",
    "\n",
    "#tweets low\n",
    "tweets = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "dense_1_tw = Dense(100, activation='relu')(hub_layer_tw)\n",
    "batch_out_tw = tf.keras.layers.BatchNormalization()(dense_1_tw) \n",
    "tw_output = Dense(32, activation='relu')(batch_out_tw)\n",
    "#tw_output = Dropout(0.5)(dense_3_tw)\n",
    "\n",
    "\n",
    "#concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "concat_layer = Concatenate()([num_output, tw_output])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "model = Model(inputs=[num_input, tweets], outputs=output)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "validation_data=([X2_test, X_tweet_test], y_test)\n",
    "x=[X2_train, X_tweet_train]\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=x, y=y_train, epochs=5, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inital preprocessing for text using GloVe. Discarded in favor of a covid-pretrained dataset\n",
    "def preprocess_text(sen):\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "X1_train = []\n",
    "sentences = list(X_train['text_tokens_str'])\n",
    "for sen in sentences:\n",
    "    X1_train.append(preprocess_text(sen))\n",
    "    \n",
    "X1_test = []\n",
    "sentences = list(X_test[\"text_tokens_str\"])\n",
    "for sen in sentences:\n",
    "    X1_test.append(preprocess_text(sen))\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X1_train)\n",
    "\n",
    "X1_train = tokenizer.texts_to_sequences(X1_train)\n",
    "X1_test = tokenizer.texts_to_sequences(X1_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\n",
    "X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('/home/aschharwood/notebooks/covid/notebooks/glove_tweets/glove.twitter.27B.100d.txt')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "#X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "X2_test = scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = Input(shape=(maxlen,))\n",
    "num_input = Input(shape=(163,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text model\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(tweet_input)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "\n",
    "#num model\n",
    "dense_layer_1 = Dense(10, activation='relu')(num_input)\n",
    "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n",
    "dense_layer_3 = Dense(10, activation='relu')(concat_layer)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer_3)\n",
    "model = Model(inputs=[tweet_input, num_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 163) for input Tensor(\"input_26:0\", shape=(None, 163), dtype=float64), but it was called on an input with incompatible shape (None, 214).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 163) for input Tensor(\"input_26:0\", shape=(None, 163), dtype=float64), but it was called on an input with incompatible shape (None, 214).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_76 is incompatible with the layer: expected axis -1 of input shape to have value 163 but received input with shape [None, 214]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-6523ffc48ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model training complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /anaconda/envs/covid_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_76 is incompatible with the layer: expected axis -1 of input shape to have value 163 but received input with shape [None, 214]\n"
     ]
    }
   ],
   "source": [
    "print('training model')\n",
    "history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=3, verbose=1, validation_data=([X1_test, X2_test], y_test))\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_env",
   "language": "python",
   "name": "conda-env-covid_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
