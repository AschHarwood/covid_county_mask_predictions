{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting County-Level Mask Wearing\n",
    "\n",
    "This model is an initial prototype that attempts to forecast mask wearing based on the interaction of static determinants of health behaviors, such as demographic, economic, and social indicators, with information exposure from news and social media. The long-term goal would be to develop a model that is responsive to a changing information environment. Specifically, can we measure how changing information in news and media influences peoples' decision to wear a mask or not at the county-level?\n",
    "\n",
    "## Integrating realtime information data to predict behavior\n",
    "\n",
    "The secondary goal of this model is to develop a methodology for integrating realtime information flows with contextual information to predict and/or forecast how different groups of people might change their attitudes, beliefs, and/or behaviors based on an evolving information ecosystem. This type of work could be useful to quickly detect any potential changes in human behavior and help, for example, public health practitioners to better allocate resources, design more targeted health communication campaigns, etc. \n",
    "\n",
    "## Feature Data\n",
    "\n",
    "This MVP uses as its feature set numeric inputs from the CDC Social Vulnerability Index, Measure of America Youth Disconnection Index, and Apple mobility data at the county-level, combined with county-level geolocated tweets and state geotagged Covid-related news. \n",
    "\n",
    "The *CDC Social Vulnerability Index* data is mostly complete, so the only preprocessing I did was to apply a MinMax scaler to scale the data since each dimensions typically represents a different kind of measurement. I also converted the county FIPS code and state in categorical variables.\n",
    "https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html\n",
    "\n",
    "For the *Measure of America Youth Disconnection Index*, there are a number of counties, particularly those that are smaller and/or more rural, that did not have data. In that case, I replaced those missing values with the mean for each county's respective state.\n",
    "http://www.measureofamerica.org/DYinteractive/#County\n",
    "\n",
    "I filtered the *Apple mobility* data for the two weeks prior to NYT face mask survey dates. For counties that lacked data, I filled those missing data with the country-wide mean for that day.\n",
    "https://covid19.apple.com/mobility\n",
    "\n",
    "Tweets from June 15 - July 15 were extracted from the *Coronvavirus Tweets Dataset*, created by Rabindra Lamsal, geolocated to the relevant US county, tokenized, and aggregated into a set of tokens for each county.\n",
    "https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset\n",
    "\n",
    "Finally, I used a dataset of U.S. political news articles I had compiled for another project, did some light \"covid\" keyword filtering for relevant text, applied some light keyword matching for state names, tokenized, and then aggregated at the state level.\n",
    "https://github.com/AschHarwood/predicting_attitudes_from_news\n",
    "\n",
    "Text data collection and processing is quite sloppy for this particular version, but I wanted to spend some time simply learning how to build multi-input Keras models.\n",
    "\n",
    "### Shape of the Data\n",
    "\n",
    "The final feature data set is composed of 3142 rows, one row for each U.S. county. You can view an overview of the data via this Pandas Profile report.\n",
    "http://htmlpreview.github.io/?https://github.com/AschHarwood/covid_county_mask_predictions/blob/main/notebooks/dataset_profile.html\n",
    "\n",
    "\n",
    "## Target Data\n",
    "\n",
    "The target for this model is binary classification about whether more or less than 50 percent of the population for each county wear's a mask. It's derived from the New York Times July 2020 survey into mask wearing.\n",
    "https://github.com/nytimes/covid-19-data/tree/master/mask-use\n",
    "\n",
    "## Model Structure\n",
    "\n",
    "The model's architecture consists of a multi-input feed forward neural net built with Keras' functional API. To preprocess the text data, I used word embeddings created by a pretrained model custom build for covid-related content.\n",
    "\n",
    "## Results\n",
    "\n",
    "The results are quite promising. With some light cross-fold validation, the model returned an average 78 percent accuracy. Decent results shouldn't be too surprising, since, theoretically, there should be a relationship between socioeconomic determinants of health, information exposure, and mask wearing. Essentially, the idea is to quantify the social ecological model, which is a framework for understanding what influences health-related behaviors.\n",
    "\n",
    "It is worth noting that the news and/or tweet models alone do not do very well...yet. On the other hand, the numeric-only model returns somewhere between 72 - 75 accuracy on a test set. So the text data, even as spotty as it is, still seems to be adding value to the model.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Secure additional relevant text data (news, television transcripts, social media posts, statements from relevant political leaders)\n",
    "- Identity an additional target mask wearing data at another point in time\n",
    "- Add additional indicators that likely have a bearing on mask wearing, such political or religious affiliation, vaccine coverage rates, etc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-hub\n",
    "#!pip install pandas-profiling[notebook]\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "pd.set_option('max_columns', 500)\n",
    "\n",
    "#!pip install talos\n",
    "\n",
    "#!pip install spacy\n",
    "\n",
    "#!pip install scikeras\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read in and process data\n",
    "# df = pd.read_csv('/home/aschharwood/notebooks/covid_county_mask_predictions/data/feature_target_text_processed_1_20_20.csv')\n",
    "\n",
    "df = pd.read_csv('/home/aschharwood/notebooks/covid_county_mask_predictions/data/feature_target_text_processed_1_20_20.csv')\n",
    "df.drop(['Unnamed: 0', 'COUNTYFP.1'], axis=1, inplace=True)\n",
    "\n",
    "#df.info(max_cols=500)\n",
    "\n",
    "mean = df._get_numeric_data().mean()\n",
    "\n",
    "# #fill empty mobility data with column mean\n",
    "df.fillna(mean, inplace=True)\n",
    "\n",
    "# #df.head()\n",
    "\n",
    "df['COUNTYFP'] = df['COUNTYFP'].astype('string')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_profile = ProfileReport(df[['text_tokens_str', 'High', 'Low']], title='Text Report', explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_profile.to_file('text_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#profile = ProfileReport(df, title=\"Pandas Profiling Report\", minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile.to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up text data\n",
    "df['text_tokens_str'] = df['text_tokens_str'].replace(r'\\n',' ', regex=True)\n",
    "df['High'] = df['High'].replace(r'\\n',' ', regex=True)\n",
    "df['Low'] = df['Low'].replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discard unneeded mask wearing columns\n",
    "df.drop(['NEVER', 'RARELY', 'SOMETIMES', 'FREQUENTLY', 'ALWAYS', 'FIPS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['always_binary']\n",
    "X = df.drop('always_binary', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab spacy's language model\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and load spacy language model\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load(disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help text tokenization function\n",
    "def tokenizer(text, nlp):\n",
    "\n",
    "    token_list = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_punct==False and token.like_url==False:\n",
    "            if token.text != ' ':\n",
    "                token_list.append((token.lemma_).lower())\n",
    "    str_tokens = ' '.join(token_list)\n",
    "    return str_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googles News Stacked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['always_binary']\n",
    "X = df.drop('always_binary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = pd.get_dummies(X[['COUNTYFP', 'ST_ABBR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, cats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['High'] = X['High'].apply(lambda x: tokenizer(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Low'] = X['Low'].apply(lambda x: tokenizer(x, nlp))\n",
    "X['text_tokens_str'] = X['text_tokens_str'].apply(lambda x: tokenizer(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3142, 220)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3142,)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X, y], axis=1).to_csv('feature_target_text_processed_1_20_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "#X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "X2_test = scaler.transform(X2_test)\n",
    "\n",
    "#download and set word vector pretrained model\n",
    "#embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "#hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "print('Google news model downlownded')\n",
    "\n",
    "X_News_High_train = X_train['High']\n",
    "X_News_High_test = X_test['High']\n",
    "\n",
    "X_News_Low_train = X_train['Low']\n",
    "X_News_Low_test = X_test['Low']\n",
    "\n",
    "X_tweet_train = X_train['text_tokens_str']\n",
    "X_tweet_test = X_test['text_tokens_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#numeric input\n",
    "num_input = Input(shape=(214,))\n",
    "dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "batch_out_num = tf.keras.layers.BatchNormalization()(dense_layer_1_num) \n",
    "num_output = Dense(10, activation='relu')(batch_out_num)\n",
    "\n",
    "#tweets low\n",
    "tweets = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "dense_1_tw = Dense(16, activation='relu')(hub_layer_tw)\n",
    "dense_2_tw = Dense(8, activation='relu')(dense_1_tw)\n",
    "batch_out_tweets = tf.keras.layers.BatchNormalization()(dense_2_tw) \n",
    "tw_output = Dense(4, activation='relu')(batch_out_tweets)\n",
    "#tw_output = Dropout(0.5)(dense_3_tw)\n",
    "\n",
    "#news low\n",
    "news_low = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_nl = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_low)\n",
    "dense_1_nl = Dense(16, activation='relu')(hub_layer_nl)\n",
    "dense_2_nl = Dense(8, activation='relu')(dense_1_nl)\n",
    "batch_out_nl = tf.keras.layers.BatchNormalization()(dense_2_nl) \n",
    "nl_output = Dense(4, activation='relu')(batch_out_nl)\n",
    "#nl_output = Dropout(0.5)(dense_3_nl)\n",
    "\n",
    "#news high\n",
    "news_high = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_news_high = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_high)\n",
    "dense_1_news_high = Dense(16, activation='relu')(hub_layer_news_high)\n",
    "dense_2_nh = Dense(8, activation='relu')(dense_1_news_high)\n",
    "batch_out_nh = tf.keras.layers.BatchNormalization()(dense_2_nh) \n",
    "nh_output = Dense(4, activation='relu')(batch_out_nh)\n",
    "#nh_output = Dropout(0.5)(dense_3_nh)\n",
    "\n",
    "\n",
    "#concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "concat_layer = Concatenate()([num_output, tw_output, nl_output, nh_output])\n",
    "# dense_1_cl = Dense(100, activation='relu')(concat_layer)\n",
    "# dense_2_cl = Dense(80, activation='relu')(dense_1_cl)\n",
    "# dense_3_cl = Dense(40, activation='relu')(dense_2_cl)\n",
    "# dense_4_cl = Dense(40, activation='relu')(dense_3_cl)\n",
    "output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "model = Model(inputs=[num_input, tweets, news_low, news_high], outputs=output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "\n",
    "validation_data=([X2_test, X_tweet_test, X_News_Low_test, X_News_High_test], y_test)\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=[X2_train, X_tweet_train, X_News_Low_train, X_News_High_train], y=y_train, epochs=5, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google news model downlownded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "#X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "X2_test = scaler.transform(X2_test)\n",
    "\n",
    "#download and set word vector pretrained model\n",
    "#embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "#hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "print('Google news model downlownded')\n",
    "\n",
    "X_News_High_train = X_train['High']\n",
    "X_News_High_test = X_test['High']\n",
    "\n",
    "X_News_Low_train = X_train['Low']\n",
    "X_News_Low_test = X_test['Low']\n",
    "\n",
    "X_tweet_train = X_train['text_tokens_str']\n",
    "X_tweet_test = X_test['text_tokens_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google news model downlownded\n",
      "Epoch 1/3\n",
      "79/79 [==============================] - 33s 419ms/step - loss: 0.6218 - accuracy: 0.7409\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 33s 420ms/step - loss: 0.5926 - accuracy: 0.8030\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 33s 423ms/step - loss: 0.5829 - accuracy: 0.8301\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.5922 - accuracy: 0.7933\n",
      "accuracy: 79.33%\n",
      "Google news model downlownded\n",
      "Epoch 1/3\n",
      "79/79 [==============================] - 35s 449ms/step - loss: 0.6193 - accuracy: 0.7501\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 36s 454ms/step - loss: 0.5893 - accuracy: 0.8078\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 36s 456ms/step - loss: 0.5907 - accuracy: 0.8102\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 0.6002 - accuracy: 0.7568\n",
      "accuracy: 75.68%\n",
      "Google news model downlownded\n",
      "Epoch 1/3\n",
      "79/79 [==============================] - 34s 435ms/step - loss: 0.6228 - accuracy: 0.7362\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 34s 435ms/step - loss: 0.5906 - accuracy: 0.8050\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 34s 428ms/step - loss: 0.5864 - accuracy: 0.8162\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.6072 - accuracy: 0.8013\n",
      "accuracy: 80.13%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-6bf1a02fc97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mcvscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f%% (+/- %.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "    X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "    #X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "    X2_test = scaler.transform(X2_test)\n",
    "\n",
    "    #download and set word vector pretrained model\n",
    "    #embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "    #hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "    embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "    print('Google news model downlownded')\n",
    "\n",
    "    X_News_High_train = X_train['High']\n",
    "    X_News_High_test = X_test['High']\n",
    "\n",
    "    X_News_Low_train = X_train['Low']\n",
    "    X_News_Low_test = X_test['Low']\n",
    "\n",
    "    X_tweet_train = X_train['text_tokens_str']\n",
    "    X_tweet_test = X_test['text_tokens_str']\n",
    "    \n",
    "    #numeric input\n",
    "    num_input = Input(shape=(214,))\n",
    "    dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "    batch_out_num = tf.keras.layers.BatchNormalization()(dense_layer_1_num) \n",
    "    num_output = Dense(10, activation='relu')(batch_out_num)\n",
    "\n",
    "    #tweets low\n",
    "    tweets = Input(shape=[], dtype=tf.string)\n",
    "    hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "    dense_1_tw = Dense(16, activation='relu')(hub_layer_tw)\n",
    "    dense_2_tw = Dense(8, activation='relu')(dense_1_tw)\n",
    "    batch_out_tweets = tf.keras.layers.BatchNormalization()(dense_2_tw) \n",
    "    tw_output = Dense(4, activation='relu')(batch_out_tweets)\n",
    "    #tw_output = Dropout(0.5)(dense_3_tw)\n",
    "\n",
    "    #news low\n",
    "    news_low = Input(shape=[], dtype=tf.string)\n",
    "    hub_layer_nl = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_low)\n",
    "    dense_1_nl = Dense(16, activation='relu')(hub_layer_nl)\n",
    "    dense_2_nl = Dense(8, activation='relu')(dense_1_nl)\n",
    "    batch_out_nl = tf.keras.layers.BatchNormalization()(dense_2_nl) \n",
    "    nl_output = Dense(4, activation='relu')(batch_out_nl)\n",
    "    #nl_output = Dropout(0.5)(dense_3_nl)\n",
    "\n",
    "    #news high\n",
    "    news_high = Input(shape=[], dtype=tf.string)\n",
    "    hub_layer_news_high = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(news_high)\n",
    "    dense_1_news_high = Dense(16, activation='relu')(hub_layer_news_high)\n",
    "    dense_2_nh = Dense(8, activation='relu')(dense_1_news_high)\n",
    "    batch_out_nh = tf.keras.layers.BatchNormalization()(dense_2_nh) \n",
    "    nh_output = Dense(4, activation='relu')(batch_out_nh)\n",
    "    #nh_output = Dropout(0.5)(dense_3_nh)\n",
    "\n",
    "\n",
    "    #concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "    concat_layer = Concatenate()([num_output, tw_output, nl_output, nh_output])\n",
    "    # dense_1_cl = Dense(100, activation='relu')(concat_layer)\n",
    "    # dense_2_cl = Dense(80, activation='relu')(dense_1_cl)\n",
    "    # dense_3_cl = Dense(40, activation='relu')(dense_2_cl)\n",
    "    # dense_4_cl = Dense(40, activation='relu')(dense_3_cl)\n",
    "    output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "    model = Model(inputs=[num_input, tweets, news_low, news_high], outputs=output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    model.fit(x=[X2_train, X_tweet_train, X_News_Low_train, X_News_High_train], y=y_train, epochs=3, callbacks=callbacks, verbose=1)\n",
    "    validation_data=([X2_test, X_tweet_test, X_News_Low_test, X_News_High_test], y_test)\n",
    "    scores = model.evaluate([X2_test, X_tweet_test, X_News_Low_test, X_News_High_test], y_test)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1]*100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.38% (+/- 1.94%)\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter Tuning with Keras Tuner and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_num= scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.77821685e-03, 1.78738771e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.34264212e-03, 2.02634647e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.79317429e-03, 4.07447947e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.42862392e-02, 2.03347661e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [1.53644232e-02, 7.97585497e-04, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [1.64589410e-02, 6.95683898e-04, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='rmsprop'):\n",
    "    num_input = Input(shape=(214,))\n",
    "    dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "    num_output = Dense(1, activation='sigmoid')(dense_layer_1_num)\n",
    "    model = Model(inputs=[num_input], outputs=num_output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=.01),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# Create hyperparameter space\n",
    "epochs = [5, 7]\n",
    "batches = [None, 100, 500]\n",
    "#optimizers = [tf.keras.optimizers.Adam(lr=.01), tf.keras.optimizers.Adam(lr=.001)]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(epochs=epochs, batch_size=batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=neural_network, cv=3, param_grid=hyperparameters)\n",
    "\n",
    "# Fit grid search\n",
    "grid_result = grid.fit(X_num, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': None, 'epochs': 7}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Num Only\n",
    "\n",
    "- this model performs reasonably well with just the static socioeconomic and demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n",
      "training model\n",
      "Epoch 1/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.6287 - val_loss: 0.6687 - val_accuracy: 0.6932\n",
      "Epoch 2/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6053 - accuracy: 0.7573 - val_loss: 0.6822 - val_accuracy: 0.7504\n",
      "Epoch 3/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5863 - accuracy: 0.8022 - val_loss: 0.6567 - val_accuracy: 0.7313\n",
      "Epoch 4/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5813 - accuracy: 0.8106 - val_loss: 0.6655 - val_accuracy: 0.7536\n",
      "Epoch 5/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5826 - accuracy: 0.8066 - val_loss: 0.6584 - val_accuracy: 0.7361\n",
      "Epoch 6/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.8142 - val_loss: 0.6980 - val_accuracy: 0.7170\n",
      "Epoch 7/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5788 - accuracy: 0.8142 - val_loss: 0.6588 - val_accuracy: 0.7345\n",
      "Epoch 8/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5768 - accuracy: 0.8177 - val_loss: 0.6725 - val_accuracy: 0.7329\n",
      "Epoch 9/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5790 - accuracy: 0.8166 - val_loss: 0.6902 - val_accuracy: 0.7234\n",
      "Epoch 10/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5791 - accuracy: 0.8078 - val_loss: 0.6897 - val_accuracy: 0.7202\n",
      "Epoch 11/15\n",
      "79/79 [==============================] - 0s 988us/step - loss: 0.5757 - accuracy: 0.8193 - val_loss: 0.6636 - val_accuracy: 0.7488\n",
      "Epoch 12/15\n",
      "79/79 [==============================] - 0s 990us/step - loss: 0.5749 - accuracy: 0.8225 - val_loss: 0.6689 - val_accuracy: 0.7297\n",
      "Epoch 13/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5751 - accuracy: 0.8237 - val_loss: 0.6897 - val_accuracy: 0.7059\n",
      "Epoch 14/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5759 - accuracy: 0.8181 - val_loss: 0.6767 - val_accuracy: 0.7297\n",
      "Epoch 15/15\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5738 - accuracy: 0.8201 - val_loss: 0.6568 - val_accuracy: 0.7504\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "num_input = Input(shape=(214,))\n",
    "dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "num_output = Dense(1, activation='sigmoid')(dense_layer_1_num)\n",
    "model = Model(inputs=[num_input], outputs=num_output)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "validation_data=([X2_test], y_test)\n",
    "x=[X2_train]\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=X_num, y=y, epochs=15, verbose=1, validation_split=0.2)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 214)]             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2150      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,161\n",
      "Trainable params: 2,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254682d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254682d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254687440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd254687440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n",
      "training model\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6692 - accuracy: 0.6403 - val_loss: 0.6575 - val_accuracy: 0.6455\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6380 - accuracy: 0.6836 - val_loss: 0.6532 - val_accuracy: 0.6455\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6268 - accuracy: 0.6976 - val_loss: 0.6560 - val_accuracy: 0.6216\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6225 - accuracy: 0.7027 - val_loss: 0.6559 - val_accuracy: 0.6375\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6201 - accuracy: 0.7083 - val_loss: 0.6590 - val_accuracy: 0.6264\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6178 - accuracy: 0.7131 - val_loss: 0.6575 - val_accuracy: 0.6312\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6171 - accuracy: 0.7139 - val_loss: 0.6578 - val_accuracy: 0.6328\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6165 - accuracy: 0.7155 - val_loss: 0.6589 - val_accuracy: 0.6264\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 15s 195ms/step - loss: 0.6160 - accuracy: 0.7163 - val_loss: 0.6581 - val_accuracy: 0.6296\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 15s 196ms/step - loss: 0.6160 - accuracy: 0.7163 - val_loss: 0.6580 - val_accuracy: 0.6296\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "tweets = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "dense_1_tw = Dense(16, activation='relu')(hub_layer_tw)\n",
    "dense_2_tw = Dense(8, activation='relu')(dense_1_tw)\n",
    "tw_output = Dense(1, activation='sigmoid')(dense_2_tw)\n",
    "\n",
    "model = Model(inputs=[tweets], outputs=tw_output)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "validation_data=([X_tweet_test], y_test)\n",
    "x=[X_tweet_train]\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=x, y=y_train, epochs=10, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets and Numeric Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550ca680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550ca680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550cad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd2550cad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined and compiled\n",
      "training model\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 16s 202ms/step - loss: 0.6497 - accuracy: 0.6486 - val_loss: 0.6471 - val_accuracy: 0.6455\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.6079 - accuracy: 0.7632 - val_loss: 0.6076 - val_accuracy: 0.7568\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 16s 200ms/step - loss: 0.5830 - accuracy: 0.8253 - val_loss: 0.6543 - val_accuracy: 0.6057\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.5771 - accuracy: 0.8333 - val_loss: 0.6339 - val_accuracy: 0.6725\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 16s 199ms/step - loss: 0.5742 - accuracy: 0.8472 - val_loss: 0.6401 - val_accuracy: 0.6614\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "embedding = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'\n",
    "\n",
    "\n",
    "#numeric input\n",
    "num_input = Input(shape=(214,))\n",
    "dense_layer_1_num = Dense(10, activation='relu')(num_input)\n",
    "batch_out_num = tf.keras.layers.BatchNormalization()(dense_layer_1_num) \n",
    "num_output = Dense(10, activation='relu')(batch_out_num)\n",
    "\n",
    "#tweets low\n",
    "tweets = Input(shape=[], dtype=tf.string)\n",
    "hub_layer_tw = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)(tweets)\n",
    "dense_1_tw = Dense(100, activation='relu')(hub_layer_tw)\n",
    "batch_out_tw = tf.keras.layers.BatchNormalization()(dense_1_tw) \n",
    "tw_output = Dense(32, activation='relu')(batch_out_tw)\n",
    "#tw_output = Dropout(0.5)(dense_3_tw)\n",
    "\n",
    "\n",
    "#concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "concat_layer = Concatenate()([num_output, tw_output])\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(concat_layer)\n",
    "model = Model(inputs=[num_input, tweets], outputs=output)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=.01)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "print('model defined and compiled')\n",
    "\n",
    "\n",
    "validation_data=([X2_test, X_tweet_test], y_test)\n",
    "x=[X2_train, X_tweet_train]\n",
    "\n",
    "print('training model')\n",
    "history = model.fit(x=x, y=y_train, epochs=5, verbose=1, validation_data=validation_data)\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #inital preprocessing for text using GloVe. Discarded in favor of a covid-pretrained dataset\n",
    "# def preprocess_text(sen):\n",
    "\n",
    "#     # Remove punctuations and numbers\n",
    "#     sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "#     # Single character removal\n",
    "#     sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "#     # Removing multiple spaces\n",
    "#     sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "#     return sentence\n",
    "\n",
    "\n",
    "\n",
    "# X1_train = []\n",
    "# sentences = list(X_train['text_tokens_str'])\n",
    "# for sen in sentences:\n",
    "#     X1_train.append(preprocess_text(sen))\n",
    "    \n",
    "# X1_test = []\n",
    "# sentences = list(X_test[\"text_tokens_str\"])\n",
    "# for sen in sentences:\n",
    "#     X1_test.append(preprocess_text(sen))\n",
    "    \n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# tokenizer.fit_on_texts(X1_train)\n",
    "\n",
    "# X1_train = tokenizer.texts_to_sequences(X1_train)\n",
    "# X1_test = tokenizer.texts_to_sequences(X1_test)\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# maxlen = 200\n",
    "\n",
    "# X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\n",
    "# X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "# from numpy import array\n",
    "# from numpy import asarray\n",
    "# from numpy import zeros\n",
    "\n",
    "# embeddings_dictionary = dict()\n",
    "\n",
    "# glove_file = open('/home/aschharwood/notebooks/covid/notebooks/glove_tweets/glove.twitter.27B.100d.txt')\n",
    "\n",
    "# for line in glove_file:\n",
    "#     records = line.split()\n",
    "#     word = records[0]\n",
    "#     vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "#     embeddings_dictionary[word] = vector_dimensions\n",
    "# glove_file.close()\n",
    "\n",
    "# embedding_matrix = zeros((vocab_size, 100))\n",
    "# for word, index in tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_dictionary.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "# X2_test = X_test.drop(['COUNTYFP', 'ST_ABBR', 'text_tokens_str', 'High', 'Low'], axis=1)\n",
    "\n",
    "# #X2_train = X_train.drop(['COUNTYFP', 'ST_ABBR'], axis=1)\n",
    "# scaler = MinMaxScaler()\n",
    "# X2_train = scaler.fit_transform(X2_train)\n",
    "\n",
    "\n",
    "# X2_test = scaler.transform(X2_test)\n",
    "\n",
    "# tweet_input = Input(shape=(maxlen,))\n",
    "# num_input = Input(shape=(163,))\n",
    "\n",
    "# #text model\n",
    "# embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(tweet_input)\n",
    "# LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "\n",
    "# #num model\n",
    "# dense_layer_1 = Dense(10, activation='relu')(num_input)\n",
    "# dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
    "\n",
    "# #concat layer takes output layers from tweet and num models, which can be passed to other models\n",
    "# concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n",
    "# dense_layer_3 = Dense(10, activation='relu')(concat_layer)\n",
    "# output = Dense(1, activation='sigmoid')(dense_layer_3)\n",
    "# model = Model(inputs=[tweet_input, num_input], outputs=output)\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#              metrics=['accuracy'])\n",
    "# print('model defined and compiled')\n",
    "\n",
    "# print('training model')\n",
    "# history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=3, verbose=1, validation_data=([X1_test, X2_test], y_test))\n",
    "# print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_env",
   "language": "python",
   "name": "conda-env-covid_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
